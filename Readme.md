# ğŸŒ Attention Is All You Need â€” English â†” Hindi Machine Translation

A complete PyTorch implementation of the Transformer architecture applied to the IITB Englishâ€“Hindi machine translation task. The project includes sentencepiece-based BPE tokenization, dataset preparation, model training, and inference.

---

## ğŸ“ Project Structure

.</br>
</br>â”œâ”€â”€ Transformer/
</br>â”‚ â”œâ”€â”€ components.py
</br>â”‚ â”œâ”€â”€ encoder.py
</br>â”‚ â”œâ”€â”€ decoder.py
</br>â”‚ â””â”€â”€ transformer.py
</br>â”‚
</br>â”œâ”€â”€ Tokenizer/
</br>â”‚ â””â”€â”€ tokenizer.py
</br>â”‚
</br>â”œâ”€â”€ Data/
</br>â”‚ â””â”€â”€ dev_test/
</br>â”‚ â”œâ”€â”€ dev.en
</br>â”‚ â”œâ”€â”€ dev.hi
</br> â”œâ”€â”€ test.en
</br>â”‚ â”œâ”€â”€ test.hi
</br>â”‚ â”œâ”€â”€ dev.all
</br>â”‚ â”œâ”€â”€ test.all
</br>â”‚ â”œâ”€â”€ bpe.vocab
</br>â”‚ â””â”€â”€ ...
</br>â”‚
</br>â”œâ”€â”€ train.py
</br>â”œâ”€â”€ inference.py
</br>â”œâ”€â”€ dev.ipynb
</br>â”œâ”€â”€ loading_collab.ipynb
</br>â””â”€â”€ README.md

---

## ğŸš€ Transformer Architecture

This project contains a from-scratch PyTorch re-implementation of the architecture described in *Attention Is All You Need* (Vaswani et al., 2017).

### ğŸ”§ Core Modules (`Transformer/`)

#### **components.py**
- **Attention**  
  - Multi-head attention  
  - Scaled dot-product: `softmax(QKáµ€ / âˆšd_k) V`  
  - Supports self-attention, cross-attention, and masked decoding attention  
- **FFN**  
  - Two-layer position-wise feed-forward network  

#### **encoder.py**
Implements the **EncoderLayer**:
- Multi-head self-attention  
- Feed-forward network  
- Residual connections  
- Layer normalization  

#### **decoder.py**
Implements the **DecoderLayer**:
- Masked self-attention  
- Encoderâ€“decoder cross-attention  
- Feed-forward network  
- Residual + LayerNorm after each block  

#### **transformer.py**
Defines the complete Transformer model:
- Token embeddings  
- Positional embeddings  
- Stacked Encoder and Decoder  
- Key methods:  
  - `embed()`  
  - `make_pad_mask()`  
  - `encode()`  
  - `decode()`  

---

## ğŸ”¤ Tokenization & Vocabulary â€” SentencePiece BPE

The project uses **SentencePiece** with **BPE (Byte-Pair Encoding)**.

### Special Tokens
| Token       | ID |
|-------------|----|
| `<pad>`     | 0  |
| `<unk>`     | 1  |
| `<bos>`     | 2  |
| `<eos>`     | 3  |
| `<start>`   | 4  |
| `<end>`     | 5  |

### `Tokenizer/tokenizer.py`
- Trains BPE model on combined `.all` corpus  
- Encodes/decodes text  
- Stores vocabulary (`bpe.vocab`)  
- Used by training & inference pipelines  

---

## ğŸ“Š Dataset â€” IITB Englishâ€“Hindi Corpus

Located in `Data/dev_test/`.

Files include:
- `dev.en`, `dev.hi`  
- `test.en`, `test.hi`  
- `dev.all`, `test.all`  
- BPE vocabulary  

### `loading_collab.ipynb`
- Loads IITB dataset  
- Preprocesses & merges files  
- Saves cleaned versions  

---

## ğŸ‹ï¸ Training

### `train.py`
Includes:
- `ParallelTextDataset`  
- `collate_fn` (handles dynamic padding & mask creation)  
- Training loop with teacher forcing  

### `dev.ipynb`
Used for:
- Experimentation (e.g., embedding_dims = 12, n_heads = 3, d_ff = 48)  
- Debugging model  
- Running small-scale tests  

---

## ğŸ” Inference

### `inference.py`
- Loads trained model  
- Encodes input using SentencePiece  
- Autoregressive decoding until `<end>` token  
- Detokenizes output  

Run:
```bash
python inference.py --text "Hello world"
